{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       A      B       C      D      E      F      G  T\n0  15.26  14.84  0.8710  5.763  3.312  2.221  5.220  1\n1  14.88  14.57  0.8811  5.554  3.333  1.018  4.956  1\n2  14.29  14.09  0.9050  5.291  3.337  2.699  4.825  1\n3  13.84  13.94  0.8955  5.324  3.379  2.259  4.805  1\n4  16.14  14.99  0.9034  5.658  3.562  1.355  5.175  1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n      <th>D</th>\n      <th>E</th>\n      <th>F</th>\n      <th>G</th>\n      <th>T</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15.26</td>\n      <td>14.84</td>\n      <td>0.8710</td>\n      <td>5.763</td>\n      <td>3.312</td>\n      <td>2.221</td>\n      <td>5.220</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14.88</td>\n      <td>14.57</td>\n      <td>0.8811</td>\n      <td>5.554</td>\n      <td>3.333</td>\n      <td>1.018</td>\n      <td>4.956</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>14.29</td>\n      <td>14.09</td>\n      <td>0.9050</td>\n      <td>5.291</td>\n      <td>3.337</td>\n      <td>2.699</td>\n      <td>4.825</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13.84</td>\n      <td>13.94</td>\n      <td>0.8955</td>\n      <td>5.324</td>\n      <td>3.379</td>\n      <td>2.259</td>\n      <td>4.805</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16.14</td>\n      <td>14.99</td>\n      <td>0.9034</td>\n      <td>5.658</td>\n      <td>3.562</td>\n      <td>1.355</td>\n      <td>5.175</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "data=pd.read_csv('seeds_dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.44110009 0.50206198 0.57063884 ... 0.48612901 0.18992304 0.34545987]\n [0.40528895 0.44638843 0.66210708 ... 0.501067   0.03381725 0.21573461]\n [0.34968744 0.34741322 0.87855172 ... 0.50391233 0.25195011 0.15136337]\n ...\n [0.24696601 0.25874793 0.72731216 ... 0.42922238 0.98070331 0.26487297]\n [0.11879981 0.16595868 0.3994755  ... 0.14753457 0.36860772 0.25897637]\n [0.16215014 0.19276446 0.54709256 ... 0.2456985  0.633196   0.26831265]]\n"
    }
   ],
   "source": [
    "def normalise_scale(y,a,b):\n",
    "    l=[]\n",
    "    for i in range(len(y)):\n",
    "        l.append(((y[i]-min(y))/(max(y)-min(y)))*(a-b)+b)\n",
    "        \n",
    "    return l  \n",
    "\n",
    "target=np.asarray(data['T'])\n",
    "data=np.asarray(data.drop('T',axis=1))\n",
    "for i in range(len(data[0])):\n",
    "    data[:,i]=normalise_scale(data[:,i],0.999,.001)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ss======= 0.06975247232205228\nvalue obtained : [0.95280146 0.01172253 0.03547601]\n[1, 0, 0]\nloss======= 0.08719569289302298\nvalue obtained : [0.94135077 0.01304292 0.04560631]\n[1, 0, 0]\nloss======= 0.07804669201350697\nvalue obtained : [0.94733941 0.01063486 0.04202573]\n[1, 0, 0]\nloss======= 0.05786252235644171\nvalue obtained : [0.96068641 0.01732201 0.02199159]\n[1, 0, 0]\nloss======= 0.0649667567278525\nvalue obtained : [0.95596735 0.01426583 0.02976683]\n[1, 0, 0]\nloss======= 0.053205242517142565\nvalue obtained : [0.96379269 0.02889364 0.00731367]\n[1, 0, 0]\nloss======= 0.011755063015239035\nvalue obtained : [0.99188512 0.00371954 0.00439535]\n[1, 0, 0]\nloss======= 1.6521477137771694\nvalue obtained : [0.31816616 0.00326203 0.67857181]\n[1, 0, 0]\nloss======= 0.29163299101897705\nvalue obtained : [0.8169768 0.0555166 0.1275066]\n[1, 0, 0]\nloss======= 0.15801000565949924\nvalue obtained : [0.89626048 0.04246686 0.06127265]\n[1, 0, 0]\nloss======= 0.060236039316358014\nvalue obtained : [0.95910719 0.03490269 0.00599012]\n[1, 0, 0]\nloss======= 0.39583009555396664\nvalue obtained : [0.76005193 0.00272226 0.23722581]\n[1, 0, 0]\nloss======= 0.08088024091291865\nvalue obtained : [0.9454806  0.03990113 0.01461828]\n[1, 0, 0]\nloss======= 0.4958303509733555\nvalue obtained : [0.7091534  0.28451242 0.00633417]\n[1, 0, 0]\nloss======= 0.5007718953715953\nvalue obtained : [0.70672855 0.00646243 0.28680902]\n[1, 0, 0]\nloss======= 0.5846901841040273\nvalue obtained : [0.66679252 0.0045606  0.32864689]\n[1, 0, 0]\nloss======= 0.07110919147499237\nvalue obtained : [0.95190586 0.02132772 0.02676642]\n[1, 0, 0]\nloss======= 0.7486887231054467\nvalue obtained : [0.59514424 0.01542199 0.38943377]\n[1, 0, 0]\nloss======= 0.05855557673477841\nvalue obtained : [0.96022501 0.00636621 0.03340878]\n[1, 0, 0]\nloss======= 0.7016589781300213\nvalue obtained : [0.61486476 0.36660557 0.01852967]\n\n[1, 0, 0]\nloss======= 0.18786609541150115\nvalue obtained : [0.87790328 0.0272506  0.09484613]\n[1, 0, 0]\nloss======= 0.016084138356806707\nvalue obtained : [0.98891324 0.00377384 0.00731292]\n[1, 0, 0]\nloss======= 0.04435532090361387\nvalue obtained : [0.96972305 0.02304449 0.00723247]\n[1, 0, 0]\nloss======= 0.08713628527249283\nvalue obtained : [0.94138953 0.05351187 0.0050986 ]\n[1, 0, 0]\nloss======= 0.6491163794406108\nvalue obtained : [0.63767075 0.35671441 0.00561484]\n[1, 0, 0]\nloss======= 0.4839940832282343\nvalue obtained : [0.71499543 0.28124583 0.00375874]\n[1, 0, 0]\nloss======= 0.04610546428665739\nvalue obtained : [0.96854738 0.02042511 0.01102751]\n[1, 0, 0]\nloss======= 0.12424550986109276\nvalue obtained : [0.91748374 0.01411832 0.06839794]\n[1, 0, 0]\nloss======= 0.02381968914077214\nvalue obtained : [0.983625   0.00441742 0.01195758]\n[1, 0, 0]\nloss======= 0.020219395442369063\nvalue obtained : [0.98608274 0.00389322 0.01002404]\n[1, 0, 0]\nloss======= 0.007433208650896344\nvalue obtained : [0.99486094 0.00139643 0.00374263]\n[1, 0, 0]\nloss======= 0.48031369204045654\nvalue obtained : [0.71682175 0.26998529 0.01319296]\n[1, 0, 0]\nloss======= 0.024788677811544445\nvalue obtained : [0.98296457 0.01110415 0.00593128]\n[1, 0, 0]\nloss======= 0.010099787282822945\nvalue obtained : [0.99302381 0.00222798 0.00474821]\n[1, 0, 0]\nloss======= 0.013665239946082784\nvalue obtained : [0.9905727  0.00607692 0.00335039]\n[1, 0, 0]\nloss======= 0.02742110037298289\nvalue obtained : [0.98117263 0.01170638 0.00712099]\n[1, 0, 0]\nloss======= 0.023065419946412202\nvalue obtained : [0.98413939 0.00839962 0.00746099]\n[1, 0, 0]\nloss======= 0.05054817796294388\nvalue obtained : [0.96556937 0.02420723 0.01022339]\n[1, 0, 0]\nloss======= 0.05229316390519599\nvalue obtained : [0.96440219 0.0148602  0.0207376 ]\n[1, 0, 0]\nloss======= 0.10229694659705399\nvalue obtained : [0.93154867 0.05380039 0.01465094]\n[1, 0, 0]\nloss======= 0.2947085323339889\nvalue obtained : [0.81523702 0.05000823 0.13475476]\n[1, 0, 0]\nloss======= 0.039409781549580164\nvalue obtained : [0.97305295 0.01100924 0.01593781]\n[1, 0, 0]\nloss======= 0.06560297020082857\nvalue obtained : [0.95554587 0.02877457 0.01567956]\n[1, 0, 0]\nloss======= 0.07246719165774865\nvalue obtained : [0.95101026 0.0380904  0.01089934]\n[1, 0, 0]\nloss======= 0.019554771862831916\nvalue obtained : [0.98653711 0.00619859 0.00726429]\n[1, 0, 0]\nloss======= 0.00734301881034739\nvalue obtained : [0.99492314 0.00254909 0.00252777]\n[1, 0, 0]\nloss======= 0.025588660022383084\nvalue obtained : [0.98241966 0.01273525 0.00484509]\n[1, 0, 0]\nloss======= 0.09276416944431118\nvalue obtained : [9.37724369e-01 8.29398636e-04 6.14462321e-02]\n[1, 0, 0]\nloss======= 0.3556759603208051\nvalue obtained : [7.81503392e-01 6.16287861e-04 2.17880320e-01]\n[1, 0, 0]\nloss======= 0.10095600320120833\nvalue obtained : [9.32414921e-01 6.21666807e-04 6.69634120e-02]\n[1, 0, 0]\nloss======= 0.0200464020914506\nvalue obtained : [0.98620098 0.00120981 0.0125892 ]\n[1, 0, 0]\nloss======= 0.29452979519428446\nvalue obtained : [0.81533802 0.00547108 0.1791909 ]\n\n[1, 0, 0]\nloss======= 0.005827527073797674\nvalue obtained : [9.95968813e-01 6.78130624e-04 3.35305620e-03]\n[1, 0, 0]\nloss======= 0.004966279740034683\nvalue obtained : [9.96563555e-01 7.02189602e-04 2.73425506e-03]\n[1, 0, 0]\nloss======= 0.008106235676265413\nvalue obtained : [0.99439694 0.00241768 0.00318537]\n[1, 0, 0]\nloss======= 0.01710392182400482\nvalue obtained : [0.98821446 0.00376796 0.00801758]\n[1, 0, 0]\nloss======= 0.012917044886871747\nvalue obtained : [0.99108655 0.00399481 0.00491864]\n[1, 0, 0]\nloss======= 1.1877017985230298\nvalue obtained : [0.43900163 0.00124996 0.55974841]\n[0, 1, 0]\nloss======= 0.036054038312926744\nvalue obtained : [2.44832159e-02 9.75318929e-01 1.97855290e-04]\n[0, 1, 0]\nloss======= 0.10426250694330154\nvalue obtained : [6.88572473e-02 9.30280372e-01 8.62380480e-04]\n[0, 1, 0]\nloss======= 0.15684307205648676\nvalue obtained : [1.02181910e-01 8.96985723e-01 8.32366961e-04]\n[0, 1, 0]\nloss======= 0.0503965512461709\nvalue obtained : [3.41891035e-02 9.65670860e-01 1.40036396e-04]\n[0, 1, 0]\nloss======= 0.29807584959663236\nvalue obtained : [0.1851655  0.81333644 0.00149806]\n[0, 1, 0]\nloss======= 0.18048571249234763\nvalue obtained : [0.11587491 0.88240587 0.00171922]\n[0, 1, 0]\nloss======= 0.07889694782924435\nvalue obtained : [5.27085928e-02 9.46781258e-01 5.10149517e-04]\n[0, 1, 0]\nloss======= 0.009743074079602992\nvalue obtained : [6.67733213e-03 9.93269369e-01 5.32993508e-05]\n[0, 1, 0]\nloss======= 0.011882284991810748\nvalue obtained : [8.13066946e-03 9.91797652e-01 7.16785348e-05]\n[0, 1, 0]\nloss======= 0.8828011982384525\nvalue obtained : [0.45535134 0.54231343 0.00233523]\n[0, 1, 0]\nloss======= 0.23664466686377797\nvalue obtained : [0.14907624 0.84871691 0.00220685]\n[0, 1, 0]\nloss======= 0.041072495816502366\nvalue obtained : [2.78828625e-02 9.71932147e-01 1.84990602e-04]\n[0, 1, 0]\nloss======= 0.012485614427290811\nvalue obtained : [8.55241565e-03 9.91382973e-01 6.46115396e-05]\n[0, 1, 0]\nloss======= 0.026809449926589585\nvalue obtained : [1.83331319e-02 9.81588703e-01 7.81652624e-05]\n[0, 1, 0]\nloss======= 0.020043370283228456\nvalue obtained : [1.37205731e-02 9.86203057e-01 7.63701269e-05]\n[0, 1, 0]\nloss======= 0.046147539396502846\nvalue obtained : [3.13288400e-02 9.68519137e-01 1.52022935e-04]\n[0, 1, 0]\nloss======= 0.07310913501288069\nvalue obtained : [4.92415955e-02 9.50587190e-01 1.71214685e-04]\n[0, 1, 0]\nloss======= 0.011239156133069963\nvalue obtained : [7.69765694e-03 9.92239877e-01 6.24660083e-05]\n[0, 1, 0]\nloss======= 0.010584484321210492\nvalue obtained : [7.25427414e-03 9.92690242e-01 5.54841351e-05]\n[0, 1, 0]\nloss======= 0.011606487640182435\nvalue obtained : [7.95548561e-03 9.91987270e-01 5.72441376e-05]\n[0, 1, 0]\nloss======= 0.01653095834133149\nvalue obtained : [1.13322706e-02 9.88607010e-01 6.07192602e-05]\n[0, 1, 0]\nloss======= 0.05086325332696093\nvalue obtained : [3.44805782e-02 9.65358523e-01 1.60899212e-04]\n[0, 1, 0]\nloss======= 0.04163403185933797\nvalue obtained : [2.83013616e-02 9.71553918e-01 1.44720211e-04]\n[0, 1, 0]\nloss======= 0.0451718468801792\nvalue obtained : [3.06090584e-02 9.69174367e-01 2.16574959e-04]\n[0, 1, 0]\nloss======= 0.010205149574292296\nvalue obtained : [6.96410412e-03 9.92951289e-01 8.46070104e-05]\n[0, 1, 0]\nloss======= 0.10313904009417052\nvalue obtained : [6.84062170e-02 9.31005090e-01 5.88693301e-04]\n\n[0, 1, 0]\nloss======= 0.01139201128832072\nvalue obtained : [7.81111157e-03 9.92134754e-01 5.41347394e-05]\n[0, 1, 0]\nloss======= 0.012218111810013591\nvalue obtained : [8.37933384e-03 9.91566811e-01 5.38553797e-05]\n[0, 1, 0]\nloss======= 0.013939173620868588\nvalue obtained : [9.54490206e-03 9.90384627e-01 7.04706583e-05]\n[0, 1, 0]\nloss======= 0.02363676062832913\nvalue obtained : [1.61688215e-02 9.83749730e-01 8.14488027e-05]\n[0, 1, 0]\nloss======= 0.7181446068537823\nvalue obtained : [0.38883882 0.60787871 0.00328247]\n[0, 1, 0]\nloss======= 0.15463515471911288\nvalue obtained : [1.01262680e-01 8.98359532e-01 3.77788040e-04]\n[0, 1, 0]\nloss======= 0.01525330140406654\nvalue obtained : [1.04496354e-02 9.89482913e-01 6.74520874e-05]\n[0, 1, 0]\nloss======= 0.011088956605117209\nvalue obtained : [7.60180522e-03 9.92343185e-01 5.50098802e-05]\n[0, 1, 0]\nloss======= 0.014474835860067568\nvalue obtained : [9.91865342e-03 9.90016973e-01 6.43736907e-05]\n[0, 1, 0]\nloss======= 0.04157299596113249\nvalue obtained : [2.82807564e-02 9.71595022e-01 1.24221130e-04]\n[0, 1, 0]\nloss======= 0.022567838201337443\nvalue obtained : [1.54418861e-02 9.84478880e-01 7.92337228e-05]\n[0, 1, 0]\nloss======= 0.040022353630000694\nvalue obtained : [2.71697391e-02 9.72639877e-01 1.90384018e-04]\n[0, 1, 0]\nloss======= 0.00844853007907749\nvalue obtained : [5.79388921e-03 9.94161039e-01 4.50722072e-05]\n[0, 1, 0]\nloss======= 0.060223999845825556\nvalue obtained : [4.07321864e-02 9.59115191e-01 1.52622712e-04]\n[0, 1, 0]\nloss======= 0.08124796206865197\nvalue obtained : [5.45600335e-02 9.45239640e-01 2.00326012e-04]\n[0, 1, 0]\nloss======= 0.013682152841484338\nvalue obtained : [9.37756267e-03 9.90561083e-01 6.13541089e-05]\n[0, 1, 0]\nloss======= 0.046928651570642646\nvalue obtained : [3.18949044e-02 9.67994898e-01 1.10197740e-04]\n[0, 1, 0]\nloss======= 0.008089413881099126\nvalue obtained : [5.53187232e-03 9.94408536e-01 5.95913562e-05]\n[0, 1, 0]\nloss======= 0.007617990982511111\nvalue obtained : [5.22298153e-03 9.94733528e-01 4.34906967e-05]\n[0, 1, 0]\nloss======= 0.019862831440557307\nvalue obtained : [1.36054658e-02 9.86326478e-01 6.80562394e-05]\n[0, 1, 0]\nloss======= 0.0421601070369627\nvalue obtained : [2.86548642e-02 9.71199708e-01 1.45427792e-04]\n[0, 1, 0]\nloss======= 0.015064245899146761\nvalue obtained : [1.03252398e-02 9.89612586e-01 6.21740741e-05]\n[0, 1, 0]\nloss======= 0.02646595826484737\nvalue obtained : [1.80795320e-02 9.81822437e-01 9.80306391e-05]\n[0, 1, 0]\nloss======= 0.010354135536835429\nvalue obtained : [7.10309495e-03 9.92848753e-01 4.81521758e-05]\n[0, 1, 0]\nloss======= 0.007648514373082934\nvalue obtained : [5.24066413e-03 9.94712482e-01 4.68536486e-05]\n[0, 1, 0]\nloss======= 0.02332664281053712\nvalue obtained : [1.59250756e-02 9.83961217e-01 1.13707723e-04]\n[0, 1, 0]\nloss======= 0.2536229759750548\nvalue obtained : [0.15843185 0.83878736 0.00278079]\n[0, 1, 0]\nloss======= 0.06469889868167797\nvalue obtained : [4.36615038e-02 9.56144851e-01 1.93644822e-04]\n[0, 1, 0]\nloss======= 6.222699424517015\nvalue obtained : [0.98272244 0.01339001 0.00388755]\n[0, 1, 0]\nloss======= 0.02094000458602652\nvalue obtained : [1.43221420e-02 9.85590323e-01 8.75355333e-05]\n[0, 1, 0]\nloss======= 0.011795453412723695\nvalue obtained : [8.07446353e-03 9.91857347e-01 6.81892845e-05]\n[0, 1, 0]\nloss======= 0.09776919487864807\nvalue obtained : [6.52589168e-02 9.34476834e-01 2.64248907e-04]\n\n[0, 1, 0]\nloss======= 0.0077469645362689835\nvalue obtained : [5.31582409e-03 9.94644605e-01 3.95710005e-05]\n[0, 1, 0]\nloss======= 0.03587206884130873\nvalue obtained : [2.43201707e-02 9.75441955e-01 2.37874124e-04]\n[0, 1, 0]\nloss======= 0.023369378713303465\nvalue obtained : [1.59706450e-02 9.83932070e-01 9.72850879e-05]\n[0, 1, 0]\nloss======= 0.01271252911318401\nvalue obtained : [8.71399295e-03 9.91227055e-01 5.89519179e-05]\n[0, 1, 0]\nloss======= 0.19543709289867278\nvalue obtained : [0.12350661 0.87330827 0.00318512]\n[0, 1, 0]\nloss======= 0.05572363550152958\nvalue obtained : [3.72286117e-02 9.62111740e-01 6.59647926e-04]\n[0, 1, 0]\nloss======= 0.11470109969192252\nvalue obtained : [0.07489309 0.92357364 0.00153327]\n[0, 1, 0]\nloss======= 2.8395688580915595\nvalue obtained : [0.85228896 0.13970264 0.00800841]\n[0, 1, 0]\nloss======= 0.016227659966328693\nvalue obtained : [1.10879752e-02 9.88814867e-01 9.71575553e-05]\n[0, 1, 0]\nloss======= 0.11854402593058172\nvalue obtained : [0.07751997 0.92111678 0.00136325]\n[0, 1, 0]\nloss======= 0.24816722441961706\nvalue obtained : [0.15574141 0.84196535 0.00229324]\n[0, 1, 0]\nloss======= 0.05763428248536331\nvalue obtained : [3.87504371e-02 9.60838402e-01 4.11161015e-04]\n[0, 0, 1]\nloss======= 0.15462333220310256\nvalue obtained : [0.0983734  0.00325971 0.89836689]\n[0, 0, 1]\nloss======= 0.13978544833026757\nvalue obtained : [0.08622617 0.0061197  0.90765413]\n[0, 0, 1]\nloss======= 0.36606013128764503\nvalue obtained : [0.2170456  0.00705589 0.77589851]\n[0, 0, 1]\nloss======= 0.14734610635062417\nvalue obtained : [9.62156211e-02 8.74505176e-04 9.02909874e-01]\n[0, 0, 1]\nloss======= 0.02494264369244314\nvalue obtained : [1.70230756e-02 1.17251689e-04 9.82859673e-01]\n[0, 0, 1]\nloss======= 0.00749782741782883\nvalue obtained : [5.12906694e-03 5.45494476e-05 9.94816384e-01]\n[0, 0, 1]\nloss======= 0.1288013352904101\nvalue obtained : [8.51951208e-02 2.13855593e-04 9.14591024e-01]\n[0, 0, 1]\nloss======= 1.1895489340602836\nvalue obtained : [0.55828023 0.00327985 0.43843992]\n[0, 0, 1]\nloss======= 0.719219337009959\nvalue obtained : [0.38855177 0.00402219 0.60742604]\n[0, 0, 1]\nloss======= 0.008797809819065852\nvalue obtained : [6.03510165e-03 4.45192825e-05 9.93920379e-01]\n[0, 0, 1]\nloss======= 0.03312933800874073\nvalue obtained : [2.25089266e-02 1.92925987e-04 9.77298147e-01]\n[0, 0, 1]\nloss======= 0.008234417846240518\nvalue obtained : [5.59698980e-03 9.44159510e-05 9.94308594e-01]\n[0, 0, 1]\nloss======= 0.03040723032051396\nvalue obtained : [2.05775959e-02 2.78528998e-04 9.79143875e-01]\n[0, 0, 1]\nloss======= 0.044273471813725554\nvalue obtained : [3.01371641e-02 8.47704431e-05 9.69778065e-01]\n[0, 0, 1]\nloss======= 0.028987613266189722\nvalue obtained : [1.97904089e-02 1.01760747e-04 9.80107830e-01]\n[0, 0, 1]\nloss======= 0.00935545973836603\nvalue obtained : [6.41203330e-03 5.16968843e-05 9.93536270e-01]\n[0, 0, 1]\nloss======= 0.3777962288001458\nvalue obtained : [2.29840989e-01 5.46705625e-04 7.69612305e-01]\n[0, 0, 1]\nloss======= 0.012627862078181012\nvalue obtained : [8.61063510e-03 1.04136206e-04 9.91285229e-01]\n[0, 0, 1]\nloss======= 0.012311856935988933\nvalue obtained : [8.40854880e-03 8.90695109e-05 9.91502382e-01]\n[0, 0, 1]\nloss======= 0.010736709726654428\nvalue obtained : [7.34526808e-03 6.92279941e-05 9.92585504e-01]\n\n[0, 0, 1]\nloss======= 0.11053053282148592\nvalue obtained : [0.07186922 0.00188339 0.92624738]\n[0, 0, 1]\nloss======= 0.034517629445508805\nvalue obtained : [2.33492608e-02 2.92583965e-04 9.76358155e-01]\n[0, 0, 1]\nloss======= 0.021005998752985505\nvalue obtained : [1.43127583e-02 1.42002685e-04 9.85545239e-01]\n[0, 0, 1]\nloss======= 0.15343531726098986\nvalue obtained : [0.09942029 0.00147273 0.89910698]\n[0, 0, 1]\nloss======= 0.008329053012089341\nvalue obtained : [5.70701042e-03 4.96159529e-05 9.94243374e-01]\n[0, 0, 1]\nloss======= 3.1056771554196847\nvalue obtained : [0.88023942 0.0035895  0.11617108]\n[0, 0, 1]\nloss======= 0.02684462889999002\nvalue obtained : [1.80860700e-02 3.49162176e-04 9.81564768e-01]\n[0, 0, 1]\nloss======= 0.057474779985684704\nvalue obtained : [3.85233700e-02 5.31993202e-04 9.60944637e-01]\n[0, 0, 1]\nloss======= 0.009876253387800052\nvalue obtained : [6.76874627e-03 5.35725104e-05 9.93177681e-01]\n[0, 0, 1]\nloss======= 0.01910849977470907\nvalue obtained : [1.30892574e-02 6.84162519e-05 9.86842326e-01]\n[0, 0, 1]\nloss======= 0.004504367055780108\nvalue obtained : [3.07175242e-03 4.55679388e-05 9.96882680e-01]\n[0, 0, 1]\nloss======= 0.008449697785943237\nvalue obtained : [5.78221731e-03 5.75487806e-05 9.94160234e-01]\n[0, 0, 1]\nloss======= 0.018562717720822002\nvalue obtained : [1.27139999e-02 7.02735066e-05 9.87215727e-01]\n[0, 0, 1]\nloss======= 0.008110313675829778\nvalue obtained : [5.55209425e-03 5.37749525e-05 9.94394131e-01]\n[0, 0, 1]\nloss======= 0.005458453651495202\nvalue obtained : [3.73478576e-03 4.15775326e-05 9.96223637e-01]\n[0, 0, 1]\nloss======= 0.019900193929734706\nvalue obtained : [1.36285235e-02 7.05417545e-05 9.86300935e-01]\n[0, 0, 1]\nloss======= 0.008259695035859205\nvalue obtained : [5.65873948e-03 5.00872147e-05 9.94291173e-01]\n[0, 0, 1]\nloss======= 0.01065753770955204\nvalue obtained : [7.32033679e-03 3.96868196e-05 9.92639976e-01]\n[0, 0, 1]\nloss======= 0.010464274191953725\nvalue obtained : [7.16585023e-03 6.11903568e-05 9.92772959e-01]\n[0, 0, 1]\nloss======= 1.0286668793076372\nvalue obtained : [0.50676513 0.003072   0.49016287]\n[0, 0, 1]\nloss======= 0.031949238129237244\nvalue obtained : [2.17989359e-02 1.03176458e-04 9.78097888e-01]\n[0, 0, 1]\nloss======= 0.23109798054224717\nvalue obtained : [0.14601627 0.0019975  0.85198623]\n[0, 0, 1]\nloss======= 0.05327399159676699\nvalue obtained : [3.58402906e-02 4.12950453e-04 9.63746759e-01]\n[0, 0, 1]\nloss======= 0.019822559550809506\nvalue obtained : [1.35253483e-02 1.20640687e-04 9.86354011e-01]\n[0, 0, 1]\nloss======= 0.04176170980600312\nvalue obtained : [0.02718744 0.00134462 0.97146794]\n[0, 0, 1]\nloss======= 0.010314455772391708\nvalue obtained : [7.05514314e-03 6.87963799e-05 9.92876060e-01]\n[0, 0, 1]\nloss======= 0.006939498763427482\nvalue obtained : [4.70310589e-03 9.54381315e-05 9.95201456e-01]\n[0, 0, 1]\nloss======= 0.01532892084441089\nvalue obtained : [1.05204291e-02 4.85211114e-05 9.89431050e-01]\n[0, 0, 1]\nloss======= 0.0072231323320534144\nvalue obtained : [4.94605371e-03 4.81274986e-05 9.95005819e-01]\n[0, 0, 1]\nloss======= 0.030092882877552105\nvalue obtained : [2.05756851e-02 6.70718517e-05 9.79357243e-01]\n[0, 0, 1]\nloss======= 0.00825200426190718\nvalue obtained : [5.66156375e-03 4.19625173e-05 9.94296474e-01]\n[0, 0, 1]\nloss======= 0.05023005411430922\nvalue obtained : [3.40640863e-02 1.53602061e-04 9.65782312e-01]\n\n[0, 0, 1]\nloss======= 0.3062192464294057\nvalue obtained : [0.189814   0.00142757 0.80875843]\n[0, 0, 1]\nloss======= 0.004789277361702118\nvalue obtained : [3.28303615e-03 3.11339293e-05 9.96685830e-01]\n[0, 0, 1]\nloss======= 0.12658638113866352\nvalue obtained : [8.32661446e-02 7.37591681e-04 9.15996264e-01]\n[0, 0, 1]\nloss======= 0.9778383771366648\nvalue obtained : [0.48639333 0.00586674 0.50773993]\n[0, 0, 1]\nloss======= 0.211211440915892\nvalue obtained : [0.13424468 0.00194374 0.86381158]\n[0, 0, 1]\nloss======= 1.4892956709085012\nvalue obtained : [0.62856662 0.01524698 0.3561864 ]\n[0, 0, 1]\nloss======= 0.10094844567573102\nvalue obtained : [0.06648457 0.00109562 0.93241981]\n[0, 0, 1]\nloss======= 4.897216212282923\nvalue obtained : [0.96129764 0.00514475 0.03355761]\n[0, 0, 1]\nloss======= 0.02704060008052567\nvalue obtained : [1.83075924e-02 2.60963429e-04 9.81431444e-01]\n[0, 0, 1]\nloss======= 5.649863591745716\nvalue obtained : [0.97660955 0.00347355 0.01991689]\n[0, 0, 1]\nloss======= 0.05065316797190773\nvalue obtained : [3.43656397e-02 1.35251905e-04 9.65499108e-01]\n[0, 0, 1]\nloss======= 0.015059922873006636\nvalue obtained : [1.01598351e-02 2.24613390e-04 9.89615552e-01]\n[0, 0, 1]\nloss======= 0.1015439157413822\nvalue obtained : [6.73260508e-02 6.38918975e-04 9.32035030e-01]\n[0, 0, 1]\nloss======= 0.8683122019955821\nvalue obtained : [0.44953555 0.00267712 0.54778733]\n[0, 0, 1]\nloss======= 0.014837147831600665\nvalue obtained : [1.01656159e-02 6.60084386e-05 9.89768376e-01]\n[0, 0, 1]\nloss======= 0.04401684405072906\nvalue obtained : [0.02898696 0.00106245 0.96995059]\n[0, 0, 1]\nloss======= 0.03823928531122325\nvalue obtained : [2.59728660e-02 1.84400300e-04 9.73842734e-01]\n[0, 0, 1]\nloss======= 0.032576233992530144\nvalue obtained : [2.20179893e-02 3.09112432e-04 9.77672898e-01]\n\n"
    }
   ],
   "source": [
    "def cost_func(ypred,y):\n",
    "    return -np.sum(np.asarray([math.log(ypred[x],(2)) if y[x]==1 else 0 for x in range(len(y)) ]))\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z)/np.sum(np.exp(z))\n",
    "\n",
    "def sigmoid(z):\n",
    "    sig=1/(1+np.exp(-z))\n",
    "    sig=np.minimum(sig, 0.9999)\n",
    "    sig=np.maximum(sig, 0.0001)\n",
    "    return sig\n",
    "\n",
    "def initialize_weights(inp_neurons,out_neurons):\n",
    "    l=[]\n",
    "    for i in range(out_neurons):\n",
    "        W=np.random.randn(inp_neurons)\n",
    "        l.append(W)\n",
    "    b=np.zeros(out_neurons)\n",
    "    return l,b\n",
    "\n",
    "def normalise_scale(y,a,b):\n",
    "    l=[]\n",
    "    for i in range(len(y)):\n",
    "        l.append(((y[i]-min(y))/(max(y)-min(y)))*(a-b)+b)\n",
    "        \n",
    "    return l  \n",
    "    \n",
    "def forwardpropogate(totalW,totalB,data,n):\n",
    "    ffout=[]\n",
    "    ffout.append(data)\n",
    "    for i in range(0,n-1):\n",
    "        params=[]\n",
    "        for j in range(len(totalW[i])):\n",
    "            \n",
    "            ###############  W(transpose).X + B  ###############\n",
    "            \n",
    "            params.append(np.dot(np.asarray(totalW[i][j]).reshape(1,len(totalW[i][j])),np.asarray(data).reshape(len(data),1)).reshape(1)[0]+totalB[i][j])\n",
    "        \n",
    "        data=params\n",
    "        if(i!=n-2):\n",
    "            ##################  activated  ###############\n",
    "    \n",
    "            data=sigmoid(np.asarray(data))\n",
    "            ffout.append(data)\n",
    "        \n",
    "    ##################  final_layer-activated  ###############\n",
    "    \n",
    "    ffout.append(softmax(np.asarray(data)))\n",
    "    return ffout\n",
    "\n",
    "def backwardpropogate(totalW,ffout,y,n):\n",
    "    totalgradW=[]\n",
    "    totalgradB=[]\n",
    "    \n",
    "    ##################  final_layer-gradient  ###############\n",
    "    \n",
    "    grad_wrt_out=-(np.asarray(y)-np.asarray(ffout[len(ffout)-1]))\n",
    "    grad_inter=grad_wrt_out\n",
    "    \n",
    "    for i in range(n-2,-1,-1):\n",
    "        \n",
    "        ##################  gradW = (gradient_at_layer) . (activated_output_previous_layer)  ###############\n",
    "        \n",
    "        totalgradW.insert(0,np.dot(np.asarray(grad_inter).reshape(len(grad_inter),1),np.asarray(ffout[i]).reshape(1,len(ffout[i]))).reshape(len(grad_inter),len(ffout[i])))\n",
    "        ##################  gradB = (gradient_at_layer)  ###############\n",
    "        \n",
    "        totalgradB.insert(0,grad_inter)\n",
    "        grad_pre=[]\n",
    "        \n",
    "        ##################  grad_at_previous_layer = (gradient_at_layer) . (weight_matrix)  ###############\n",
    "        \n",
    "        for j in range(len(totalW[i][0])):\n",
    "            grad_pre.append(np.dot(np.asarray(grad_inter).reshape(1,len(grad_inter)),np.asarray(np.matrix(totalW[i])[:,j])).reshape(1)[0])\n",
    "        \n",
    "        ####  grad_at_previous_layer = (grad_at_previous_layer) . (hadamard) . (derivative_of_activation)  ####\n",
    "        \n",
    "        grad_inter=np.asarray(grad_pre)*(np.asarray(ffout[i]))*(1-np.asarray(ffout[i]))\n",
    "    \n",
    "    return totalgradW,totalgradB\n",
    "                   \n",
    "    \n",
    "totalW=[]\n",
    "totalB=[]\n",
    "max_iterations=50\n",
    "w,b=initialize_weights(len(data[0]),5)\n",
    "totalW.append(w)\n",
    "totalB.append(b)\n",
    "w,b=initialize_weights(5,3)\n",
    "totalW.append(w)\n",
    "totalB.append(b)\n",
    "#w,b=initialize_weights(3,3)\n",
    "#totalW.append(w)\n",
    "#totalB.append(b)\n",
    "no_layers=3\n",
    "\n",
    "cnt=0\n",
    "while(cnt<max_iterations):\n",
    "    cnt+=1\n",
    "    for i in range(len(data)):\n",
    "        y_one_hot=[1 if x==target[i] else 0 for x in set(target)]\n",
    "        print(y_one_hot)\n",
    "     \n",
    "        ffout=forwardpropogate(totalW,totalB,data[i],no_layers)\n",
    "        C=cost_func(ffout[len(ffout)-1],y_one_hot)\n",
    "        print('loss=======',C)\n",
    "        print('value obtained :',ffout[len(ffout)-1])\n",
    "        \n",
    "        gradW,gradB=backwardpropogate(totalW,ffout,y_one_hot,no_layers)\n",
    "        \n",
    "        if(i==0):\n",
    "            totalgradW1=gradW\n",
    "            totalgradB1=gradB\n",
    "        else:\n",
    "            for j in range(len(gradW)):\n",
    "                totalgradW1[j]=np.asarray(totalgradW1[j])+np.asarray(gradW[j])\n",
    "                totalgradB1[j]=np.asarray(totalgradB1[j])+np.asarray(gradB[j])\n",
    "        if((i+1) % 32 == 0):\n",
    "            for i in range(len(totalW)):\n",
    "                totalW[i]=np.asarray(totalW[i])-(0.05*np.asarray(totalgradW1[i]))\n",
    "                totalB[i]-=0.05*totalgradB1[i]\n",
    "            \n",
    "            for j in range(len(gradW)):\n",
    "                totalgradW1[j]=0\n",
    "                totalgradB1[j]=0\n",
    "                \n",
    "            print()\n",
    "\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Expected Output : [0, 0, 1]\nPredicted Output : [1.83075924e-02 2.60963429e-04 9.81431444e-01]\n"
    }
   ],
   "source": [
    "ffout=forwardpropogate(totalW,totalB,data[200],no_layers)\n",
    "y_one_hot=[1 if x==target[200] else 0 for x in set(target)]\n",
    "print('Expected Output :',y_one_hot)\n",
    "print('Predicted Output :',ffout[len(ffout)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def cost_func(ypred,y):\n",
    "    return -np.sum(np.asarray([math.log(ypred[x],(2)) if y[x]==1 else 0 for x in range(len(y)) ]))\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z)/np.sum(np.exp(z))\n",
    "\n",
    "def sigmoid(z):\n",
    "    sig=1/(1+np.exp(-z))\n",
    "    sig=np.minimum(sig, 0.9999)\n",
    "    sig=np.maximum(sig, 0.0001)\n",
    "    return sig\n",
    "\n",
    "def initialize_weights(inp_neurons,out_neurons):\n",
    "    l=[]\n",
    "    for i in range(out_neurons):\n",
    "        W=np.random.randn(inp_neurons)\n",
    "        l.append(W)\n",
    "    b=np.zeros(out_neurons)\n",
    "    return l,b\n",
    "\n",
    "def normalise_scale(y,a,b):\n",
    "    l=[]\n",
    "    for i in range(len(y)):\n",
    "        l.append(((y[i]-min(y))/(max(y)-min(y)))*(a-b)+b)\n",
    "        \n",
    "    return l  \n",
    "\n",
    "def gradient_checking(gradW,totalW,gradB,totalB,no_layers,data,y_one_hot):\n",
    "    E=0.2\n",
    "    for i in range(len(gradW)):\n",
    "        for j in range(len(gradW[i])):\n",
    "            for k in range(len(gradW[i][j])):\n",
    "                totalW[i][j][k]+=E\n",
    "                ffout1=forwardpropogate(totalW,totalB,data,no_layers)\n",
    "                \n",
    "                totalW[i][j][k]-=(2*E)\n",
    "                ffout2=forwardpropogate(totalW,totalB,data,no_layers)\n",
    "                \n",
    "                C1=cost_func(ffout1[len(ffout1)-1],y_one_hot)\n",
    "                C2=cost_func(ffout2[len(ffout2)-1],y_one_hot)\n",
    "                \n",
    "                C=(C1-C2)/(2*E)\n",
    "                print(C)\n",
    "                \n",
    "                if(abs(C-gradW[i][j][k]>0.3)):\n",
    "                    print(C,gradW[i][j][k])\n",
    "                    print(i,j,k)\n",
    "                    return False\n",
    "                \n",
    "                totalW[i][j][k]+=E\n",
    "    \n",
    "    for i in range(len(gradB)):\n",
    "        for j in range(len(gradB[i])):\n",
    "            totalB[i][j]+=E\n",
    "            ffout1=forwardpropogate(totalW,totalB,data,no_layers)\n",
    "\n",
    "            totalB[i][j]-=(2*E)\n",
    "            ffout2=forwardpropogate(totalW,totalB,data,no_layers)\n",
    "\n",
    "            C1=cost_func(ffout1[len(ffout1)-1],y_one_hot)\n",
    "            C2=cost_func(ffout2[len(ffout2)-1],y_one_hot)\n",
    "                \n",
    "            C=(C1-C2)/(2*E)\n",
    "                \n",
    "            if(abs(C-gradB[i][j]>0.3)):\n",
    "                print(C,gradB[i][j])\n",
    "                return False\n",
    "\n",
    "            totalB[i][j]+=E\n",
    "                      \n",
    "    return True\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def forwardpropogate(totalW,totalB,data,n):\n",
    "    ffout=[]\n",
    "    ffout.append(data)\n",
    "    for i in range(0,n-1):\n",
    "        params=[]\n",
    "        for j in range(len(totalW[i])):\n",
    "            \n",
    "            ###############  W(transpose).X + B  ###############\n",
    "            \n",
    "            params.append(np.dot(np.asarray(totalW[i][j]).reshape(1,len(totalW[i][j])),np.asarray(data).reshape(len(data),1)).reshape(1)[0]+totalB[i][j])\n",
    "        \n",
    "        data=params\n",
    "        if(i!=n-2):\n",
    "            ##################  activated  ###############\n",
    "    \n",
    "            data=sigmoid(np.asarray(data))\n",
    "            ffout.append(data)\n",
    "        \n",
    "    ##################  final_layer-activated  ###############\n",
    "    \n",
    "    ffout.append(softmax(np.asarray(data)))\n",
    "    return ffout\n",
    "\n",
    "def backwardpropogate(totalW,ffout,y,n):\n",
    "    totalgradW=[]\n",
    "    totalgradB=[]\n",
    "    \n",
    "    ##################  final_layer-gradient  ###############\n",
    "    \n",
    "    grad_wrt_out=-(np.asarray(y)-np.asarray(ffout[len(ffout)-1]))\n",
    "    grad_inter=grad_wrt_out\n",
    "    \n",
    "    for i in range(n-2,-1,-1):\n",
    "        \n",
    "        ##################  gradW = (gradient_at_layer) . (activated_output_previous_layer)  ###############\n",
    "        \n",
    "        totalgradW.insert(0,np.dot(np.asarray(grad_inter).reshape(len(grad_inter),1),np.asarray(ffout[i]).reshape(1,len(ffout[i]))).reshape(len(grad_inter),len(ffout[i])))\n",
    "        ##################  gradB = (gradient_at_layer)  ###############\n",
    "        \n",
    "        totalgradB.insert(0,grad_inter)\n",
    "        grad_pre=[]\n",
    "        \n",
    "        ##################  grad_at_previous_layer = (gradient_at_layer) . (weight_matrix)  ###############\n",
    "        \n",
    "        for j in range(len(totalW[i][0])):\n",
    "            grad_pre.append(np.dot(np.asarray(grad_inter).reshape(1,len(grad_inter)),np.asarray(np.matrix(totalW[i])[:,j])).reshape(1)[0])\n",
    "        \n",
    "        ####  grad_at_previous_layer = (grad_at_previous_layer) . (hadamard) . (derivative_of_activation)  ####\n",
    "        \n",
    "        grad_inter=np.asarray(grad_pre)*(np.asarray(ffout[i]))*(1-np.asarray(ffout[i]))\n",
    "    \n",
    "    return totalgradW,totalgradB\n",
    "                   \n",
    "    \n",
    "totalW=[]\n",
    "totalB=[]\n",
    "max_iterations=50\n",
    "w,b=initialize_weights(len(data[0]),5)\n",
    "totalW.append(w)\n",
    "totalB.append(b)\n",
    "w,b=initialize_weights(5,3)\n",
    "totalW.append(w)\n",
    "totalB.append(b)\n",
    "#w,b=initialize_weights(3,3)\n",
    "#totalW.append(w)\n",
    "#totalB.append(b)\n",
    "no_layers=3\n",
    "\n",
    "cnt=0\n",
    "while(cnt<max_iterations):\n",
    "    cnt+=1\n",
    "    for i in range(len(data)):\n",
    "        y_one_hot=[1 if x==target[i] else 0 for x in set(target)]\n",
    "        print(y_one_hot)\n",
    "     \n",
    "        ffout=forwardpropogate(totalW,totalB,data[i],no_layers)\n",
    "        C=cost_func(ffout[len(ffout)-1],y_one_hot)\n",
    "        print('loss=======',C)\n",
    "        print('value obtained :',ffout[len(ffout)-1])\n",
    "        \n",
    "        gradW,gradB=backwardpropogate(totalW,ffout,y_one_hot,no_layers)\n",
    "        \n",
    "        #val=gradient_checking(gradW,totalW,gradB,totalB,no_layers,data[i],y_one_hot)\n",
    "        \n",
    "        #if(val==False):\n",
    "         #   print('false result')\n",
    "          #  break\n",
    "        \n",
    "        if(i==0):\n",
    "            totalgradW1=gradW\n",
    "            totalgradB1=gradB\n",
    "        else:\n",
    "            for j in range(len(gradW)):\n",
    "                totalgradW1[j]=np.asarray(totalgradW1[j])+np.asarray(gradW[j])\n",
    "                totalgradB1[j]=np.asarray(totalgradB1[j])+np.asarray(gradB[j])\n",
    "        if((i+1) % 32 == 0):\n",
    "            for i in range(len(totalW)):\n",
    "                totalW[i]=np.asarray(totalW[i])-(0.05*np.asarray(totalgradW1[i]))\n",
    "                totalB[i]-=0.05*totalgradB1[i]\n",
    "            \n",
    "            for j in range(len(gradW)):\n",
    "                totalgradW1[j]=0\n",
    "                totalgradB1[j]=0\n",
    "                \n",
    "            print()\n",
    "\n",
    "    print()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python_defaultSpec_1594895887486"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}